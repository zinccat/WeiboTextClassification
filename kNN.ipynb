{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### kNN分类器"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "本程序实现了一个简单的kNN分类器,请确认dataset目录中已存在kfold程序生成的csv文件,且已下载预训练词向量"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "导入所用到的库"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #用于数据处理\n",
        "import pandas as pd #用于数据处理\n",
        "import time #用于测试计时\n",
        "from tqdm import tqdm #用于绘制进度条"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:54:04.674Z",
          "iopub.status.busy": "2020-03-27T14:54:04.670Z",
          "iopub.status.idle": "2020-03-27T14:54:04.938Z",
          "shell.execute_reply": "2020-03-27T14:54:04.935Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下函数被用于读取预训练词向量并建立词典\n",
        "\n",
        "本实验中所使用的预训练词向量来自https://github.com/Embedding/Chinese-Word-Vectors\n",
        "\n",
        "可以在Word2vec - Weibo 微博 - Word类别下下载\n",
        "\n",
        "维度=300 使用前请务必删除第一行的两个矩阵大小参数\n",
        "\n",
        "输入参数: file: 预训练词向量的地址"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pretrained_vecs(file):\n",
        "    print(\"Loading pretrained word vectors...\")\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        words = {'UNK'} #设置UNK避免使用时出现不在词典中的词\n",
        "        mapping = {'UNK': np.zeros((1,300))} #将UNK映射到全0向量\n",
        "        for sentence in f:\n",
        "            sentence = sentence.strip().split()\n",
        "            curr_word = sentence[0] #取出预训练词向量中的词汇\n",
        "            words.add(curr_word)\n",
        "            mapping[curr_word] = np.array(sentence[1:], dtype=np.float64) #将向量加入字典\n",
        "    return words, mapping"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:54:07.014Z",
          "iopub.status.busy": "2020-03-27T14:54:07.010Z",
          "iopub.status.idle": "2020-03-27T14:54:07.018Z",
          "shell.execute_reply": "2020-03-27T14:54:07.031Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用pandas库中函数从用kfold程序预处理得到的csv文件中读取训练集与测试集\n",
        "\n",
        "传入参数: data_id: 所使用文件的编号"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_id):\n",
        "    train_set = pd.read_csv('./dataset/Train'+str(data_id)+'UTF8.csv', encoding=\"utf-8\")\n",
        "    test_set = pd.read_csv('./dataset/Test'+str(data_id)+'UTF8.csv', encoding=\"utf-8\")\n",
        "    return train_set, test_set"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:54:09.039Z",
          "iopub.status.busy": "2020-03-27T14:54:09.036Z",
          "iopub.status.idle": "2020-03-27T14:54:09.044Z",
          "shell.execute_reply": "2020-03-27T14:54:09.052Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "将训练集中的句子通过预训练词向量嵌入到句子矩阵中\n",
        "\n",
        "传入参数: sentence_length: 截取或补全语句后语句长度"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def build_train_set(sentence_length=50):\n",
        "    print(\"Now building the train set...\")\n",
        "    for idx1, sentence in enumerate(train_set['text']):\n",
        "        splited = str(sentence).split() #分词\n",
        "        for idx2, word in enumerate(splited):\n",
        "            key = mapping.get(word, np.zeros((300))) #取出词典中单词对应词向量,使用零向量作为默认参数\n",
        "            for i in range(300): #将预训练词向量中参数复制到语句矩阵\n",
        "                train_sentences[idx1][300*idx2+i] = key[i]\n",
        "            if idx2==sentence_length-1: #截断过长语句\n",
        "                break\n",
        "        norm = np.linalg.norm(train_sentences[idx1]) #计算语句向量范数\n",
        "        if norm != 0: #判断是否为零向量, 避免除0情况出现\n",
        "            train_sentences[idx1] = train_sentences[idx1]/norm #归一化"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T15:05:12.511Z",
          "iopub.status.busy": "2020-03-27T15:05:12.509Z",
          "iopub.status.idle": "2020-03-27T15:05:12.515Z",
          "shell.execute_reply": "2020-03-27T15:05:12.523Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "类似于build_train_set函数\n",
        "\n",
        "将测试集中的句子通过预训练词向量嵌入到句子矩阵中\n",
        "\n",
        "传入参数: sentence_length: 截取或补全语句后语句长度"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def build_test_set(sentence_length=50):\n",
        "    print(\"Now building the test set...\")\n",
        "    for idx1, sentence in enumerate(test_set['text']):\n",
        "        splited = str(sentence).split() #分词\n",
        "        for idx2, word in enumerate(splited):\n",
        "            key = mapping.get(word, np.zeros((300))) #取出词典中单词对应词向量,使用零向量作为默认参数\n",
        "            for i in range(300): #将预训练词向量中参数复制到语句矩阵\n",
        "                test_sentences[idx1][300*idx2+i] = key[i]\n",
        "            if idx2==sentence_length-1: #截断过长语句\n",
        "                break\n",
        "        norm = np.linalg.norm(test_sentences[idx1]) #计算语句向量范数\n",
        "        if norm != 0: #判断是否为零向量, 避免除0情况出现\n",
        "            test_sentences[idx1] = test_sentences[idx1]/norm #归一化"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T15:05:14.720Z",
          "iopub.status.busy": "2020-03-27T15:05:14.717Z",
          "iopub.status.idle": "2020-03-27T15:05:14.724Z",
          "shell.execute_reply": "2020-03-27T15:05:14.732Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "相似度衡量函数 (已弃用, 仅用于说明原理)\n",
        "\n",
        "使用向量内积计算两个语句向量的余弦相似度\n",
        "\n",
        "传入参数: vec1, vec2: 待比较的两个语句向量"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(vec1, vec2):\n",
        "    dot = np.dot(vec1, vec2) #计算向量点积\n",
        "    product = np.linalg.norm(vec1) * np.linalg.norm(vec2) #计算原长度乘积\n",
        "    if product == 0: #判断是否有零向量参与比较, 避免除0情况出现\n",
        "        return 0\n",
        "    else:\n",
        "        return dot/product #返回余弦相似度"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:55:52.954Z",
          "iopub.status.busy": "2020-03-27T14:55:52.951Z",
          "iopub.status.idle": "2020-03-27T14:55:52.960Z",
          "shell.execute_reply": "2020-03-27T14:55:52.962Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "将相似度函数应用到全部训练集数据上，得到相似度向量\n",
        "\n",
        "传入参数: vec: 语句向量"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def getsim(vec):\n",
        "    sim = np.zeros((train_data_len,))\n",
        "    for i in range(train_data_len):\n",
        "        out = np.dot(vec,train_sentences[i]) #直接计算已归一化的语句向量间的余弦相似度\n",
        "        sim[i] = out\n",
        "    return sim #返回传入语句与整个训练集中语句的相似度构成的向量"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:55:56.252Z",
          "iopub.status.busy": "2020-03-27T14:55:56.250Z",
          "iopub.status.idle": "2020-03-27T14:55:56.258Z",
          "shell.execute_reply": "2020-03-27T14:55:56.261Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "kNN算法主体函数\n",
        "\n",
        "传入参数: vec: 语句向量 k: 所使用的近邻数"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def kNN(vec, k):\n",
        "    sim = []\n",
        "    sim = getsim(vec) #得到所有训练集中语句与传入语句的相似度\n",
        "    nearest = np.argpartition(-sim, k)[0:k] #部分排序后取k个近邻的编号\n",
        "    return np.argmax(np.bincount(train_labels[nearest])) #返回k个近邻中出现次数最多的标签"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:55:57.616Z",
          "iopub.status.busy": "2020-03-27T14:55:57.614Z",
          "iopub.status.idle": "2020-03-27T14:55:57.622Z",
          "shell.execute_reply": "2020-03-27T14:55:57.624Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试所用函数, 可传入参数k作为近邻个数"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test(k=16):\n",
        "    print(\"Testing...\")\n",
        "    start = time.time()\n",
        "    correct = 0\n",
        "    for idx in tqdm(range(test_data_len)):\n",
        "        predict = kNN(test_sentences[idx],k) #得到预测类别\n",
        "        if predict == test_labels[idx]:\n",
        "            correct += 1\n",
        "        #else: #输出分类错误的语句\n",
        "            #print(\"sentence: %s, predicted label: %d, correct label: %d\" % (test_set['text'][idx], predict, test_labels[idx]))\n",
        "        #if(idx % 100 == 99):\n",
        "        #    print('Tested: %d, Correct: %d' %(idx+1,correct))#每100个数据输出正确个数\n",
        "    end = time.time()\n",
        "    print('Accuracy: %.3f' %(correct / test_data_len)) #计算正确率\n",
        "    print('Test Time: %dseconds' % (end - start)) #输出测试所用时间"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:55:58.880Z",
          "iopub.status.busy": "2020-03-27T14:55:58.876Z",
          "iopub.status.idle": "2020-03-27T14:55:58.884Z",
          "shell.execute_reply": "2020-03-27T14:55:58.893Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "读取预训练词向量, weibo为文件名(测试时将其放到了根目录)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "words, mapping = read_pretrained_vecs('./weibo')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained word vectors...\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T14:56:00.532Z",
          "iopub.status.busy": "2020-03-27T14:56:00.528Z",
          "iopub.status.idle": "2020-03-27T14:56:18.455Z",
          "shell.execute_reply": "2020-03-27T14:56:18.449Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "初始化测试所需参数"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = read_data(5) #在read_data括号内输入想要测试的数据集的编号\n",
        "sentence_length = 50 #截取或补全语句后语句长度\n",
        "train_data_len = len(train_set) #得到训练集数据总数\n",
        "test_data_len = len(test_set) #得到测试集数据总数\n",
        "train_sentences = np.zeros((train_data_len, 300*sentence_length)) #生成空训练集矩阵\n",
        "train_labels = np.array(train_set['category']) #生成训练集编号数组\n",
        "build_train_set() #构建训练集矩阵\n",
        "test_sentences = np.zeros((test_data_len, 300*sentence_length))\n",
        "test_labels = np.array(test_set['category']) #生成测试集编号数组\n",
        "build_test_set() #构建测试集矩阵"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now building the train set...\n",
            "Now building the test set...\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T15:05:18.094Z",
          "iopub.status.busy": "2020-03-27T15:05:18.091Z",
          "iopub.status.idle": "2020-03-27T15:06:29.195Z",
          "shell.execute_reply": "2020-03-27T15:06:29.205Z"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(16)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|                                                                                         | 0/1364 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████| 1364/1364 [02:54<00:00,  7.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.765\n",
            "Test Time: 174seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-03-27T15:06:57.975Z",
          "iopub.status.busy": "2020-03-27T15:06:57.970Z",
          "iopub.status.idle": "2020-03-27T15:09:57.331Z",
          "shell.execute_reply": "2020-03-27T15:09:57.339Z"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit ('pytorch': conda)",
      "language": "python",
      "name": "python37664bitpytorchcondaf7d82aec40484e15b0f1651e739ca307"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.22.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}